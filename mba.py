"""
Instacart MBA Recommendation System V1
=====================================
å•†å“çµ„åˆåˆ†æèˆ‡æ¨è–¦ç³»çµ±

åŠŸèƒ½ï¼š
1. train_mba_model(): è¨“ç·´é—œè¯è¦å‰‡æ¨¡å‹ (ä½¿ç”¨ PySpark FPGrowth åŠ é€Ÿ)
2. recommend_products(): åŸºæ–¼è³¼ç‰©ç±ƒæ¨è–¦å•†å“
"""

import os
import json
import pickle
import warnings
from typing import List, Union, Dict, Set, Tuple

import numpy as np
import pandas as pd

warnings.filterwarnings('ignore')

# ============================================================
# å…¨åŸŸé…ç½®
# ============================================================
DATA_PATH = "./data/"
MODEL_PATH = "./model/mba/"

# é è¨­åƒæ•¸
DEFAULT_PARAMS = {
    'mba_algorithm': 'FPGrowth',
    'min_support': 0.0005,
    'min_confidence': 0.2,
    'max_k': 2,
    'recommend_top_n': 3
}


# ============================================================
# Phase 1: è³‡æ–™å‰è™•ç†èˆ‡æ•´åˆ
# ============================================================
def load_product_mappings(data_path: str = DATA_PATH) -> Tuple[Dict, Dict, pd.DataFrame]:
    """
    è¼‰å…¥å•†å“æ˜ å°„å­—å…¸ï¼ˆä¸éœ€è¦ Sparkï¼Œç”¨ Pandas å³å¯ï¼‰ã€‚
    
    Args:
        data_path: è³‡æ–™ç›®éŒ„è·¯å¾‘
        
    Returns:
        Tuple[Dict, Dict, pd.DataFrame]:
            - id_to_name: product_id -> product_name æ˜ å°„
            - name_to_id: product_name -> product_id æ˜ å°„
            - products_full: å®Œæ•´å•†å“è³‡æ–™è¡¨
    """
    print("\n[1.1] è¼‰å…¥å•†å“å°ç…§è¡¨...")
    products = pd.read_csv(os.path.join(data_path, "products.csv"))
    departments = pd.read_csv(os.path.join(data_path, "departments.csv"))
    aisles = pd.read_csv(os.path.join(data_path, "aisles.csv"))
    
    products_full = products.merge(departments, on='department_id', how='left')
    products_full = products_full.merge(aisles, on='aisle_id', how='left')
    
    id_to_name = dict(zip(products_full['product_id'], products_full['product_name']))
    name_to_id = dict(zip(products_full['product_name'], products_full['product_id']))
    
    print(f"   âœ“ å•†å“æ•¸é‡: {len(id_to_name):,}")
    
    return id_to_name, name_to_id, products_full


# ============================================================
# Phase 2: ä½¿ç”¨ PySpark FPGrowth è¨“ç·´æ¨¡å‹
# ============================================================
def train_mba_model(
    data_path: str = DATA_PATH,
    model_path: str = MODEL_PATH,
    min_support: float = 0.001,
    min_confidence: float = 0.1,
    limit_rows: int = None,  # None = ä½¿ç”¨å…¨éƒ¨è³‡æ–™
    spark_driver_memory: str = "8g",
    spark_executor_memory: str = "8g"
) -> pd.DataFrame:
    """
    ä½¿ç”¨ PySpark FPGrowth è¨“ç·´é—œè¯è¦å‰‡æ¨¡å‹ï¼ˆå¯è™•ç†å…¨éƒ¨ 3300 è¬ç­†äº¤æ˜“ï¼‰ã€‚
    
    Args:
        data_path: è³‡æ–™ç›®éŒ„è·¯å¾‘
        model_path: æ¨¡å‹å„²å­˜è·¯å¾‘
        min_support: æœ€å°æ”¯æŒåº¦é–¾å€¼
        min_confidence: æœ€å°ä¿¡è³´åº¦é–¾å€¼
        limit_rows: é™åˆ¶è³‡æ–™ç­†æ•¸ (None è¡¨ç¤ºå…¨éƒ¨)
        spark_driver_memory: Spark Driver è¨˜æ†¶é«”
        spark_executor_memory: Spark Executor è¨˜æ†¶é«”
        
    Returns:
        pd.DataFrame: é—œè¯è¦å‰‡ DataFrame
    """
    import time
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import collect_list, col, size
    from pyspark.ml.fpm import FPGrowth
    
    print("\n" + "=" * 60)
    print("ğŸ”§ Phase 2: æ¨¡å‹è¨“ç·´èˆ‡è¦å‰‡ç”Ÿæˆ (PySpark FPGrowth)")
    print("=" * 60)
    print(f"\nåƒæ•¸è¨­å®š:")
    print(f"   min_support: {min_support}")
    print(f"   min_confidence: {min_confidence}")
    print(f"   limit_rows: {'å…¨éƒ¨è³‡æ–™' if limit_rows is None else f'{limit_rows:,}'}")
    print(f"   spark_driver_memory: {spark_driver_memory}")
    
    # 1. è¼‰å…¥å•†å“æ˜ å°„ï¼ˆä½¿ç”¨ Pandasï¼‰
    id_to_name, name_to_id, products_full = load_product_mappings(data_path)
    
    # 2. åˆå§‹åŒ– Spark Session
    print("\n[2.1] åˆå§‹åŒ– Spark Session...")
    
    # Windows å…¼å®¹æ€§è¨­ç½®
    import os as os_module
    os_module.environ['PYSPARK_PYTHON'] = 'python'
    os_module.environ['PYSPARK_DRIVER_PYTHON'] = 'python'
    
    spark = SparkSession.builder \
        .appName("MBA_FPGrowth_Full") \
        .master("local[*]") \
        .config("spark.driver.memory", spark_driver_memory) \
        .config("spark.executor.memory", spark_executor_memory) \
        .config("spark.sql.shuffle.partitions", "200") \
        .config("spark.driver.extraJavaOptions", "-Djava.security.manager=allow") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    print("   âœ“ Spark Session å·²åˆå§‹åŒ–")
    
    try:
        # 3. è¼‰å…¥äº¤æ˜“è³‡æ–™
        print("\n[2.2] è¼‰å…¥äº¤æ˜“è³‡æ–™...")
        df_prior = spark.read.csv(
            os.path.join(data_path, "order_products__prior.csv"),
            header=True, inferSchema=True
        )
        df_train = spark.read.csv(
            os.path.join(data_path, "order_products__train.csv"),
            header=True, inferSchema=True
        )
        
        # åˆä½µè³‡æ–™
        df_total = df_prior.union(df_train)
        
        # è¨ˆç®—ç¸½ç­†æ•¸
        if limit_rows:
            print(f"   âš ï¸ é™åˆ¶è³‡æ–™ç‚º {limit_rows:,} ç­†")
            df_total = df_total.limit(limit_rows)
        
        total_count = df_total.count()
        print(f"   âœ“ ç¸½äº¤æ˜“ç­†æ•¸: {total_count:,}")
        
        # 4. å»ºç«‹è³¼ç‰©ç±ƒï¼ˆä»¥ order_id åˆ†çµ„ï¼‰
        print("\n[2.3] å»ºç«‹è³¼ç‰©ç±ƒè³‡æ–™...")
        basket_data = df_total.select("order_id", "product_id") \
            .groupBy("order_id") \
            .agg(collect_list("product_id").alias("items"))
        
        # éæ¿¾åªæœ‰ 1 å€‹å•†å“çš„è¨‚å–®
        basket_data = basket_data.filter(size(col("items")) >= 2)
        basket_count = basket_data.count()
        print(f"   âœ“ æœ‰æ•ˆè³¼ç‰©ç±ƒæ•¸: {basket_count:,}")
        
        # 5. è¨“ç·´ FPGrowth æ¨¡å‹
        print(f"\n[2.4] è¨“ç·´ FPGrowth æ¨¡å‹...")
        start_time = time.time()
        
        fpGrowth = FPGrowth(
            itemsCol="items",
            minSupport=min_support,
            minConfidence=min_confidence
        )
        model = fpGrowth.fit(basket_data)
        
        training_time = time.time() - start_time
        print(f"   âœ“ è¨“ç·´å®Œæˆï¼è€—æ™‚: {training_time:.2f} ç§’")
        
        # 6. å–å¾—é »ç¹é …é›†èˆ‡é—œè¯è¦å‰‡
        print("\n[2.5] æå–é—œè¯è¦å‰‡...")
        
        # ç›´æ¥å–å¾—é—œè¯è¦å‰‡ï¼ˆé¿å…è¨ˆç®—å…¨éƒ¨é »ç¹é …é›†ä»¥ç¯€çœè¨˜æ†¶é«”ï¼‰
        rules_spark = model.associationRules
        
        # å…ˆå–å‰ N æ¢è¦å‰‡ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
        print("   â†’ æŒ‰ Lift æ’åºä¸¦å–å¾—è¦å‰‡...")
        rules_spark = rules_spark.sort(col("lift").desc())
        
        # é™åˆ¶è¦å‰‡æ•¸é‡ï¼ˆæœ€å¤š 10000 æ¢ï¼‰
        rules_spark = rules_spark.limit(10000)
        
        # 7. è½‰æ›ç‚º Pandas DataFrame
        print("\n[2.6] è½‰æ›è¦å‰‡æ ¼å¼...")
        rules_pd = rules_spark.toPandas()
        
        rules_count = len(rules_pd)
        print(f"   âœ“ é—œè¯è¦å‰‡æ•¸é‡: {rules_count:,}")
        
        if rules_count == 0:
            print("   âš ï¸ æœªç”Ÿæˆä»»ä½•è¦å‰‡ï¼Œè«‹å˜—è©¦é™ä½ min_support æˆ– min_confidence")
            return pd.DataFrame()
        
        # å°‡ antecedent å’Œ consequent è½‰ç‚º frozensetï¼ˆèˆ‡ mlxtend æ ¼å¼ç›¸å®¹ï¼‰
        rules_pd['antecedents'] = rules_pd['antecedent'].apply(lambda x: frozenset(x))
        rules_pd['consequents'] = rules_pd['consequent'].apply(lambda x: frozenset(x))
        
        # é‡æ–°å‘½åæ¬„ä½ä»¥ä¿æŒç›¸å®¹æ€§
        rules_pd = rules_pd.rename(columns={'support': 'support'})
        
        # æŒ‰ Lift æ’åº
        rules_pd = rules_pd.sort_values('lift', ascending=False).reset_index(drop=True)
        
        print(f"\n   è¦å‰‡çµ±è¨ˆ:")
        print(f"   â†’ Support ç¯„åœ: [{rules_pd['support'].min():.6f}, {rules_pd['support'].max():.6f}]")
        print(f"   â†’ Confidence ç¯„åœ: [{rules_pd['confidence'].min():.4f}, {rules_pd['confidence'].max():.4f}]")
        print(f"   â†’ Lift ç¯„åœ: [{rules_pd['lift'].min():.2f}, {rules_pd['lift'].max():.2f}]")
        
        # 8. å„²å­˜æ¨¡å‹
        print(f"\n[2.7] å„²å­˜æ¨¡å‹...")
        os.makedirs(model_path, exist_ok=True)
        
        # å„²å­˜é—œè¯è¦å‰‡
        rules_path = os.path.join(model_path, "mba_rules_model.pkl")
        with open(rules_path, 'wb') as f:
            pickle.dump(rules_pd, f)
        print(f"   âœ“ è¦å‰‡å·²å„²å­˜: {rules_path}")
        
        # å„²å­˜å•†å“æ˜ å°„å­—å…¸
        mappings = {
            'id_to_name': id_to_name,
            'name_to_id': name_to_id
        }
        mappings_path = os.path.join(model_path, "product_mappings.pkl")
        with open(mappings_path, 'wb') as f:
            pickle.dump(mappings, f)
        print(f"   âœ“ å•†å“æ˜ å°„å·²å„²å­˜: {mappings_path}")
        
        # å„²å­˜åƒæ•¸è¨­å®š
        params = {
            'min_support': min_support,
            'min_confidence': min_confidence,
            'limit_rows': limit_rows,
            'total_transactions': total_count,
            'basket_count': basket_count,
            'n_rules': len(rules_pd),
            'n_products': len(id_to_name),
            'training_time_seconds': training_time
        }
        params_path = os.path.join(model_path, "training_params.json")
        with open(params_path, 'w', encoding='utf-8') as f:
            json.dump(params, f, indent=2, ensure_ascii=False)
        print(f"   âœ“ è¨“ç·´åƒæ•¸å·²å„²å­˜: {params_path}")
        
        print("\n" + "=" * 60)
        print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        print("=" * 60)
        
        # é¡¯ç¤ºå‰ 10 æ¢è¦å‰‡
        print("\nğŸ“‹ å‰ 10 æ¢è¦å‰‡ (æŒ‰ Lift æ’åº):")
        print("-" * 80)
        for idx, row in rules_pd.head(10).iterrows():
            ant_names = [id_to_name.get(pid, str(pid)) for pid in row['antecedents']]
            con_names = [id_to_name.get(pid, str(pid)) for pid in row['consequents']]
            print(f"{idx+1:2}. {ant_names} â†’ {con_names}")
            print(f"    Support: {row['support']:.6f} | Confidence: {row['confidence']:.4f} | Lift: {row['lift']:.2f}")
        
        return rules_pd
        
    finally:
        # ç¢ºä¿ Spark Session è¢«é—œé–‰
        spark.stop()
        print("\n   âœ“ Spark Session å·²é—œé–‰")


# ============================================================
# Phase 3: æ¨è–¦æŸ¥è©¢å‡½å¼
# ============================================================
def recommend_products(
    input_list: List[Union[int, str]],
    model_path: str = MODEL_PATH,
    top_n: int = DEFAULT_PARAMS['recommend_top_n']
) -> List[str]:
    """
    åŸºæ–¼è³¼ç‰©ç±ƒæ¨è–¦å•†å“ã€‚
    
    å¯æ¥å—å•†å“ ID (int) æˆ–å•†å“åç¨± (str) çš„æ··åˆè¼¸å…¥ã€‚
    
    Args:
        input_list: è³¼ç‰©ç±ƒå•†å“åˆ—è¡¨ (å¯æ··åˆ ID å’Œåç¨±)
        model_path: æ¨¡å‹ç›®éŒ„è·¯å¾‘
        top_n: æ¨è–¦å•†å“æ•¸é‡
        
    Returns:
        List[str]: æ¨è–¦çš„å•†å“åç¨±åˆ—è¡¨
    """
    # 1. è¼‰å…¥æ¨¡å‹å’Œæ˜ å°„
    rules_path = os.path.join(model_path, "mba_rules_model.pkl")
    mappings_path = os.path.join(model_path, "product_mappings.pkl")
    
    if not os.path.exists(rules_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {rules_path}ï¼Œè«‹å…ˆåŸ·è¡Œ train_mba_model()")
    
    with open(rules_path, 'rb') as f:
        rules = pickle.load(f)
    
    with open(mappings_path, 'rb') as f:
        mappings = pickle.load(f)
    
    id_to_name = mappings['id_to_name']
    name_to_id = mappings['name_to_id']
    
    # 2. è§£æè¼¸å…¥ï¼Œçµ±ä¸€è½‰æ›ç‚º product_id é›†åˆ
    input_ids: Set[int] = set()
    
    for item in input_list:
        if isinstance(item, int):
            # è¼¸å…¥æ˜¯ product_id
            if item in id_to_name:
                input_ids.add(item)
            else:
                print(f"   âš ï¸ æœªçŸ¥çš„å•†å“ ID: {item}")
        elif isinstance(item, str):
            # è¼¸å…¥æ˜¯ product_name
            if item in name_to_id:
                input_ids.add(name_to_id[item])
            else:
                # å˜—è©¦æ¨¡ç³ŠåŒ¹é…
                matches = [name for name in name_to_id.keys() if item.lower() in name.lower()]
                if matches:
                    matched_name = matches[0]
                    input_ids.add(name_to_id[matched_name])
                    print(f"   â„¹ï¸ æ¨¡ç³ŠåŒ¹é…: '{item}' â†’ '{matched_name}'")
                else:
                    print(f"   âš ï¸ æœªçŸ¥çš„å•†å“åç¨±: {item}")
    
    if not input_ids:
        print("   âŒ ç„¡æ³•è­˜åˆ¥ä»»ä½•è¼¸å…¥å•†å“")
        return []
    
    # 3. ç¯©é¸è¦å‰‡
    # æ¢ä»¶ 1: antecedent å¿…é ˆæ˜¯ input_ids çš„å­é›†
    # æ¢ä»¶ 2: consequent ä¸èƒ½åŒ…å« input_ids ä¸­çš„ä»»ä½•å•†å“
    
    candidate_products: Dict[int, float] = {}  # product_id -> max_lift
    
    for _, row in rules.iterrows():
        antecedent = set(row['antecedents'])
        consequent = set(row['consequents'])
        
        # æª¢æŸ¥æ¢ä»¶ 1: antecedent âŠ† input_ids
        if not antecedent.issubset(input_ids):
            continue
        
        # æª¢æŸ¥æ¢ä»¶ 2: consequent âˆ© input_ids = âˆ…
        if consequent.intersection(input_ids):
            continue
        
        # ç¬¦åˆæ¢ä»¶ï¼Œè¨˜éŒ„æ¨è–¦å•†å“èˆ‡å…¶ Lift
        lift = row['lift']
        for product_id in consequent:
            if product_id not in candidate_products or candidate_products[product_id] < lift:
                candidate_products[product_id] = lift
    
    # 4. æŒ‰ Lift æ’åºä¸¦å– top_n
    sorted_products = sorted(candidate_products.items(), key=lambda x: x[1], reverse=True)
    top_products = sorted_products[:top_n]
    
    # 5. è½‰æ›ç‚ºå•†å“åç¨±
    recommendations = [id_to_name.get(pid, f"Unknown({pid})") for pid, _ in top_products]
    
    return recommendations


def recommend_products_verbose(
    input_list: List[Union[int, str]],
    model_path: str = MODEL_PATH,
    top_n: int = DEFAULT_PARAMS['recommend_top_n']
) -> Dict:
    """
    æ¨è–¦å•†å“ï¼ˆè©³ç´°ç‰ˆæœ¬ï¼ŒåŒ…å«å®Œæ•´è³‡è¨Šï¼‰ã€‚
    
    Args:
        input_list: è³¼ç‰©ç±ƒå•†å“åˆ—è¡¨
        model_path: æ¨¡å‹ç›®éŒ„è·¯å¾‘
        top_n: æ¨è–¦å•†å“æ•¸é‡
        
    Returns:
        Dict: åŒ…å«æ¨è–¦çµæœå’Œè©³ç´°è³‡è¨Šçš„å­—å…¸
    """
    # è¼‰å…¥æ¨¡å‹å’Œæ˜ å°„
    rules_path = os.path.join(model_path, "mba_rules_model.pkl")
    mappings_path = os.path.join(model_path, "product_mappings.pkl")
    
    with open(rules_path, 'rb') as f:
        rules = pickle.load(f)
    
    with open(mappings_path, 'rb') as f:
        mappings = pickle.load(f)
    
    id_to_name = mappings['id_to_name']
    name_to_id = mappings['name_to_id']
    
    # è§£æè¼¸å…¥
    input_ids: Set[int] = set()
    input_parsed = []
    
    for item in input_list:
        if isinstance(item, int):
            if item in id_to_name:
                input_ids.add(item)
                input_parsed.append({
                    'original': item,
                    'product_id': item,
                    'product_name': id_to_name[item]
                })
        elif isinstance(item, str):
            if item in name_to_id:
                pid = name_to_id[item]
                input_ids.add(pid)
                input_parsed.append({
                    'original': item,
                    'product_id': pid,
                    'product_name': item
                })
            else:
                matches = [name for name in name_to_id.keys() if item.lower() in name.lower()]
                if matches:
                    matched_name = matches[0]
                    pid = name_to_id[matched_name]
                    input_ids.add(pid)
                    input_parsed.append({
                        'original': item,
                        'product_id': pid,
                        'product_name': matched_name,
                        'fuzzy_match': True
                    })
    
    # ç¯©é¸è¦å‰‡ä¸¦æ”¶é›†è©³ç´°è³‡è¨Š
    candidate_products: Dict[int, Dict] = {}
    matched_rules = []
    
    for _, row in rules.iterrows():
        antecedent = set(row['antecedents'])
        consequent = set(row['consequents'])
        
        if not antecedent.issubset(input_ids):
            continue
        if consequent.intersection(input_ids):
            continue
        
        matched_rules.append({
            'antecedent': [id_to_name.get(pid, str(pid)) for pid in antecedent],
            'consequent': [id_to_name.get(pid, str(pid)) for pid in consequent],
            'support': float(row['support']),
            'confidence': float(row['confidence']),
            'lift': float(row['lift'])
        })
        
        lift = row['lift']
        confidence = row['confidence']
        for product_id in consequent:
            if product_id not in candidate_products or candidate_products[product_id]['lift'] < lift:
                candidate_products[product_id] = {
                    'product_id': int(product_id),
                    'product_name': id_to_name.get(product_id, f"Unknown({product_id})"),
                    'lift': float(lift),
                    'confidence': float(confidence)
                }
    
    # æ’åºä¸¦å– top_n
    sorted_products = sorted(candidate_products.values(), key=lambda x: x['lift'], reverse=True)
    recommendations = sorted_products[:top_n]
    
    return {
        'input': input_parsed,
        'input_product_ids': list(input_ids),
        'matched_rules_count': len(matched_rules),
        'recommendations': recommendations,
        'top_matched_rules': matched_rules[:5]  # åªé¡¯ç¤ºå‰ 5 æ¢åŒ¹é…è¦å‰‡
    }


# ============================================================
# æ¸¬è©¦å‡½å¼
# ============================================================
def run_test_cases(model_path: str = MODEL_PATH) -> Dict:
    """
    åŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹ï¼Œé©—è­‰æ¨è–¦å‡½å¼ã€‚
    
    Returns:
        Dict: æ¸¬è©¦çµæœ
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª Phase 3: æ¨è–¦æŸ¥è©¢æ¸¬è©¦")
    print("=" * 60)
    
    # æ¸¬è©¦æ¡ˆä¾‹ï¼šæ··åˆ ID å’Œåç¨±
    test_cases = [
        {
            "name": "æ¸¬è©¦æ¡ˆä¾‹ 1: ç´”å•†å“ ID (é¦™è•‰ç›¸é—œ)",
            "input": [24852],  # Banana
            "top_n": 5
        },
        {
            "name": "æ¸¬è©¦æ¡ˆä¾‹ 2: ç´”å•†å“åç¨± (æœ‰æ©Ÿè”¬æœ)",
            "input": ["Organic Cilantro"],  # é¦™èœ
            "top_n": 3
        },
        {
            "name": "æ¸¬è©¦æ¡ˆä¾‹ 3: æ··åˆè¼¸å…¥ (ID + åç¨±)",
            "input": ["Organic Raspberries", 21137],  # è¦†ç›†å­ + Organic Strawberries
            "top_n": 5
        }
    ]
    
    results = {}
    
    for case in test_cases:
        print(f"\nğŸ“Œ {case['name']}")
        print(f"   è¼¸å…¥: {case['input']}")
        
        try:
            # ä½¿ç”¨è©³ç´°ç‰ˆæœ¬
            result = recommend_products_verbose(
                input_list=case['input'],
                model_path=model_path,
                top_n=case['top_n']
            )
            
            print(f"   è§£æå¾Œå•†å“:")
            for item in result['input']:
                print(f"      â†’ {item['original']} â†’ {item['product_name']} (ID: {item['product_id']})")
            
            print(f"   åŒ¹é…è¦å‰‡æ•¸: {result['matched_rules_count']}")
            print(f"   æ¨è–¦çµæœ:")
            for i, rec in enumerate(result['recommendations'], 1):
                print(f"      {i}. {rec['product_name']} (Lift: {rec['lift']:.2f})")
            
            results[case['name']] = {
                'input': case['input'],
                'parsed_input': result['input'],
                'recommendations': [r['product_name'] for r in result['recommendations']],
                'matched_rules_count': result['matched_rules_count']
            }
            
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤: {e}")
            results[case['name']] = {'error': str(e)}
    
    # å„²å­˜æ¸¬è©¦çµæœ
    output_path = os.path.join(model_path, "recommendation_test_output.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… æ¸¬è©¦çµæœå·²å„²å­˜: {output_path}")
    
    return results


# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ›’ Instacart MBA Recommendation System V1 (PySpark)")
    print("=" * 60)
    
    # Phase 1 & 2: ä½¿ç”¨ PySpark FPGrowth è¨“ç·´æ¨¡å‹ï¼ˆå…¨éƒ¨è³‡æ–™ï¼‰
    rules = train_mba_model(
        data_path=DATA_PATH,
        model_path=MODEL_PATH,
        min_support=0.005,       # 0.5% æ”¯æŒåº¦ï¼ˆç´„ 16000 ç­†è¨‚å–®ï¼‰
        min_confidence=0.1,      # 10% ä¿¡è³´åº¦
        limit_rows=None,         # None = ä½¿ç”¨å…¨éƒ¨ 3300 è¬ç­†è³‡æ–™
        spark_driver_memory="12g",
        spark_executor_memory="12g"
    )
    
    # Phase 3: åŸ·è¡Œæ¸¬è©¦
    if len(rules) > 0:
        test_results = run_test_cases(MODEL_PATH)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“ è¼¸å‡ºæª”æ¡ˆ:")
        print(f"   â†’ {MODEL_PATH}mba_rules_model.pkl")
        print(f"   â†’ {MODEL_PATH}product_mappings.pkl")
        print(f"   â†’ {MODEL_PATH}training_params.json")
        print(f"   â†’ {MODEL_PATH}recommendation_test_output.json")
